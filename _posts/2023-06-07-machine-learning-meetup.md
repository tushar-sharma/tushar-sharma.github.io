---
published: false
---
Large Language model. It's based on transformer. It's a set of layers that helps it to understand to text. 

1. the model 
2. the tokenizer (split our text into shorter text).

Run a pipeline . it's a chain of inferences. Convert text into array of token. Then model use token and generate ? . 

Hugging face provides layer transformer. It's work with pytorch, etc. it's agnostic.

Optimum Intel. It has
1. neural compressor
2. openVINO




[^ref1]: https://github.com/openvinotoolkit/openvino_notebooks
[^ref2]: https://huggingface.co/docs/transformers/index
[^ref3]: https://huggingface.co/docs/optimum/index
[^ref4]: https://huggingface.co/databricks/dolly-v2-3b
